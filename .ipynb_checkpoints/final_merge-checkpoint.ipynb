{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings, sys\n",
    "\n",
    "pd.set_option('display.max_columns', 100)\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weather = pd.read_csv('../data/vzw.csv')\n",
    "env = pd.read_csv('../data/vz311/vz311intersectionsLighter.txt', delimiter='\\t', encoding='utf8', engine='python')\n",
    "env.columns = [i.replace(' ', '_') for i in env.columns]\n",
    "weather['UNIQUE KEY'] = weather['UNIQUE KEY'].fillna(0)\n",
    "weather['UNIQUE KEY'] = weather['UNIQUE KEY'].astype(str)\n",
    "env['UNIQUE_KEY'] = env.UNIQUE_KEY.fillna(1)\n",
    "env['UNIQUE_KEY'] = env.UNIQUE_KEY.astype(str)\n",
    "# minute index\n",
    "# env['time_stamp'] = pd.to_datetime(env.TIME, format='%H:%m:%s')\n",
    "# env['time_idx'] = env.TIME.apply(lambda x: str(x).split(':')[1])\n",
    "# weather['time_idx'] = weather.TIME.apply(lambda x: str(x).split(':')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(277255, 35)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# weather\n",
    "# weather.isnull().sum()\n",
    "# env.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# env.head()\n",
    "# weather=None\n",
    "# env=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# assign labels\n",
    "env['label'] = np.where(env.DATE.isnull(), 0, 1)\n",
    "\n",
    "# change the nans to 0s\n",
    "env['NUMBER_OF_CYCLIST_INJURED'] = np.where(env.NUMBER_OF_CYCLIST_INJURED.isnull(), 0, env.NUMBER_OF_CYCLIST_INJURED)\n",
    "env['NUMBER_OF_CYCLIST_KILLED'] = np.where(env.NUMBER_OF_CYCLIST_KILLED.isnull(), 0, env.NUMBER_OF_CYCLIST_KILLED)\n",
    "env['NUMBER_OF_MOTORIST_INJURED'] = np.where(env.NUMBER_OF_MOTORIST_INJURED.isnull(), 0, env.NUMBER_OF_MOTORIST_INJURED)\n",
    "env['NUMBER_OF_MOTORIST_KILLED'] = np.where(env.NUMBER_OF_MOTORIST_KILLED.isnull(), 0, env.NUMBER_OF_MOTORIST_KILLED)\n",
    "env['NUMBER_OF_PEDESTRIANS_INJURED'] = np.where(env.NUMBER_OF_PEDESTRIANS_INJURED.isnull(), 0, env.NUMBER_OF_PEDESTRIANS_INJURED)\n",
    "env['NUMBER_OF_PEDESTRIANS_KILLED'] = np.where(env.NUMBER_OF_PEDESTRIANS_KILLED.isnull(), 0, env.NUMBER_OF_PEDESTRIANS_KILLED)\n",
    "env['NUMBER_OF_PERSONS_INJURED'] = np.where(env.NUMBER_OF_PERSONS_INJURED.isnull(), 0, env.NUMBER_OF_PERSONS_INJURED)\n",
    "env['NUMBER_OF_PERSONS_KILLED'] = np.where(env.NUMBER_OF_PERSONS_KILLED.isnull(), 0, env.NUMBER_OF_PERSONS_KILLED)\n",
    "\n",
    "# assign speical vehicle code type for nans\n",
    "codes = np.arange(1,6, dtype=np.int64)\n",
    "for i in codes:\n",
    "    car_type = 'VEHICLE_TYPE_CODE_%d' % i\n",
    "    env[car_type] = np.where(env[car_type].isnull(), 'CONTROL', env[car_type])\n",
    "    \n",
    "# assign special contributing factor for nans\n",
    "for j in codes:\n",
    "    fac_type = 'CONTRIBUTING_FACTOR_VEHICLE_%d' % j\n",
    "    env[fac_type] = np.where(env[fac_type].isnull(), 'CONTROL', env[fac_type])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = env.merge(weather, how='left', left_on=['UNIQUE_KEY'], right_on=['UNIQUE KEY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# weather2.head()\n",
    "# env.head()\n",
    "# df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### call the full weather dataset and randomly fill the NAN in the merged df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fw = pd.read_csv('../data/weather_raw_NYC/full_weather.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(83)\n",
    "scores = np.random.random_integers(0, fw.shape[0]-1, size=df.DATE_x.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eidx = df[df.DATE_x.isnull()].index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gg = fw.iloc[scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gg['new_index'] = eidx\n",
    "gg.columns = ['new_%s' % j for j in gg.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2 = df.merge(gg, how='left', left_index=True, right_on='new_new_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# replace the nan values for null dates in full file\n",
    "df2['Visibility'] = np.where(df2.DATE_x.isnull(), df2.new_Visibility, df2.Visibility)\n",
    "df2['WetBulbFarenheit'] = np.where(df2.DATE_x.isnull(), df2.new_WetBulbFarenheit, df2.WetBulbFarenheit)\n",
    "df2['WindSpeed'] = np.where(df2.DATE_x.isnull(), df2.new_WindSpeed, df2.WindSpeed)\n",
    "df2['Precip'] = np.where(df2.DATE_x.isnull(), df2.new_Precip, df2.Precip)\n",
    "df2['PrecipSum'] = np.where(df2.DATE_x.isnull(), df2.new_PrecipSum, df2.PrecipSum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for i, v in enumerate(eidx):\n",
    "#     sys.stdout.write('\\rline %d' % i)\n",
    "#     sys.stdout.flush()\n",
    "#     idx = scores[i]\n",
    "#     ww = fw.iloc[idx]\n",
    "#     df.iloc[v]['DATE'] = ww.Date\n",
    "#     df.iloc[v]['Visibility'] = ww.Visibility\n",
    "#     df.iloc[v]['WetBulbFarenheit'] = ww.WetBulbFarenheit\n",
    "#     df.iloc[v]['WindSpeed'] = ww.WindSpeed\n",
    "#     df.iloc[v]['Precip'] = ww.Precip\n",
    "#     df.iloc[v]['PrecipSum'] = ww.PrecipSum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in all the data and give it an index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df = pd.read_csv('../data/vz_full.csv')\n",
    "df2['identity'] = ['id-%d' % d for d in np.arange(0, df2.shape[0], dtype=np.int64)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# subset the columns\n",
    "cc = ['identity', 'label', 'NUMBER_OF_PERSONS_INJURED', 'NUMBER_OF_PERSONS_KILLED',\n",
    "      'NUMBER_OF_PEDESTRIANS_INJURED', 'NUMBER_OF_PEDESTRIANS_KILLED','NUMBER_OF_CYCLIST_INJURED', \n",
    "      'NUMBER_OF_CYCLIST_KILLED','NUMBER_OF_MOTORIST_INJURED', 'NUMBER_OF_MOTORIST_KILLED',\n",
    "      'CONTRIBUTING_FACTOR_VEHICLE_1', 'CONTRIBUTING_FACTOR_VEHICLE_2', 'CONTRIBUTING_FACTOR_VEHICLE_3',\n",
    "      'CONTRIBUTING_FACTOR_VEHICLE_4', 'CONTRIBUTING_FACTOR_VEHICLE_5', 'VEHICLE_TYPE_CODE_1',\n",
    "      'VEHICLE_TYPE_CODE_2', 'VEHICLE_TYPE_CODE_3', 'VEHICLE_TYPE_CODE_4', 'VEHICLE_TYPE_CODE_5',\n",
    "      'Street_Condition', 'Traffic_Signal_Condition', 'Visibility', 'WetBulbFarenheit',\n",
    "      'WindSpeed', 'Precip', 'PrecipSum','intersectionID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# subset\n",
    "d1 = df2[cc]\n",
    "\n",
    "# remove the column spaces with _\n",
    "d1.columns = [x.replace(' ', '_') for x in d1.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # take the string out of cyclist value\n",
    "# d1['NUMBER_OF_CYCLIST_INJURED'] = np.where(d1.NUMBER_OF_CYCLIST_INJURED=='PASSENGER VEHICLE', \n",
    "#                                            np.nan, d1.NUMBER_OF_CYCLIST_INJURED).astype(np.float64)\n",
    "# d1['NUMBER_OF_CYCLIST_KILLED'] = np.where(d1.NUMBER_OF_CYCLIST_KILLED=='PASSENGER VEHICLE', \n",
    "#                                           np.nan, d1.NUMBER_OF_CYCLIST_KILLED).astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# total persons involved in accident [injured + killed]\n",
    "d1['total_involved'] = np.sum(d1.values[:,1:9].astype(np.float64), axis=1).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# d1.to_csv('temp_data.csv')\n",
    "d1 = pd.read_csv('temp_data.csv').drop(['Unnamed: 0'],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dummy out the strings\n",
    "d2 = pd.DataFrame(d1.identity)\n",
    "for head in d1.columns:\n",
    "    if head != 'identity':\n",
    "        if d1[head].dtype == np.float64:\n",
    "            d2 = d2.merge(pd.DataFrame(d1[head]), left_index=True, right_index=True)\n",
    "        elif d1[head].dtype == object:\n",
    "            temp = pd.get_dummies(d1[head], prefix=head, prefix_sep='-', dummy_na=True)\n",
    "            d2 = d2.merge(temp, right_index=True, left_index=True)\n",
    "        elif d1[head].dtype == np.int64:\n",
    "            d2[head] =d1[head]\n",
    "        else:\n",
    "            print('Skipped a Column', head)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2 (py27)",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
